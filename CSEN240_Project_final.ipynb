{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjOvZG_gFMQn"
      },
      "source": [
        "#### Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmn9gCJMme4R",
        "outputId": "378546a3-689f-4010-edb6-ba1225d87c1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2_3Ef5pFMQo"
      },
      "source": [
        "#### Setting up base paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XdI7eOZmqRv"
      },
      "outputs": [],
      "source": [
        "# NOT TO BE CHANGED IN SUBMISSION\n",
        "BASE_INPUT_PATH = \"/content/drive/MyDrive/SP24-CSEN240-2/Project\"\n",
        "TRAIN_DATA_PATH = BASE_INPUT_PATH + \"/Train\"\n",
        "VALIDATION_DATA_PATH = BASE_INPUT_PATH + \"/Validation\"\n",
        "TEST_DATA_PATH = BASE_INPUT_PATH + \"/Test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0UgwQuLTe-H"
      },
      "outputs": [],
      "source": [
        "GROUP_NUMBER = \"4\" # TODO\n",
        "ADDITIONAL_FILES_PATH = BASE_INPUT_PATH + \"/Temp/Group-%s\" % GROUP_NUMBER\n",
        "OUTPUT_PATH = BASE_INPUT_PATH + \"/Prediction\"\n",
        "\n",
        "AUGMENTATION_DATA_PATH = ADDITIONAL_FILES_PATH + \"/Augmentation\" #point to folder where augmented data will be saved\n",
        "MODEL_PATH = \"/content\" #point to where model will be save and read\n",
        "\n",
        "PREDICTION_FILE_PATH = OUTPUT_PATH + \"/group-4.txt\"\n",
        "TEST_LABEL_PATH = TEST_DATA_PATH+\"/labels.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlZ6QdIT0bkK"
      },
      "source": [
        "#### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6f9sZzt0mho",
        "outputId": "ea7fc353-8ccd-4551-c261-8978707065ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Collecting libsvm-official\n",
            "  Downloading libsvm-official-3.32.0.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from libsvm-official) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->libsvm-official) (1.25.2)\n",
            "Building wheels for collected packages: libsvm-official\n",
            "  Building wheel for libsvm-official (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libsvm-official: filename=libsvm_official-3.32.0-cp310-cp310-linux_x86_64.whl size=123887 sha256=37360fa37d7b74ef40f9c80bdc1958ae85abe038b5f9f0ad1db736600e6edbd8\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/3b/1b/73bb4869517f96a26c82b47ccdb9ec48f12f4466de2371eff6\n",
            "Successfully built libsvm-official\n",
            "Installing collected packages: libsvm-official\n",
            "Successfully installed libsvm-official-3.32.0\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python scikit-learn numpy\n",
        "!pip install -U libsvm-official"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGczuObjRCRm"
      },
      "source": [
        "#### Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ya627TtRB-I",
        "outputId": "6107780f-7350-4c22-9734-11cb901fdc9e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/libsvm/svm.py:147: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def csr_to_problem_jit(l, x_val, x_ind, x_rowptr, prob_val, prob_ind, prob_rowptr, indx_start):\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import scipy\n",
        "from collections import Counter\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from libsvm.svmutil import *\n",
        "from joblib import dump, load\n",
        "from sklearn.decomposition import PCA\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import array_to_img, img_to_array, load_img\n",
        "from os import listdir\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from statistics import mode as statistics_mode\n",
        "import random\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0sU6IGC05bj"
      },
      "source": [
        "#### Define Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMk1nieR09Xi"
      },
      "outputs": [],
      "source": [
        "def write_predictions_to_file(y_pred, path):\n",
        "\n",
        "    with open(path, 'w') as file:\n",
        "        for idx, pred in enumerate(y_pred, start=1):\n",
        "            file.write(f\"{idx:03d} {pred}\\n\")\n",
        "    print(f\"Predictions written to {path}\")\n",
        "\n",
        "def hard_voting(y_svm_pred, y_rf_pred, y_logreg_pred, y_knn_pred):\n",
        "\n",
        "    final_predictions = []\n",
        "\n",
        "    # Iterate over the predictions from each classifier\n",
        "    for predictions in zip(y_svm_pred, y_rf_pred, y_logreg_pred, y_knn_pred):\n",
        "        # Count the occurrences of each prediction\n",
        "        counts = Counter(predictions)\n",
        "        # Find the maximum occurrence\n",
        "        max_count = max(counts.values())\n",
        "        # Collect all predictions that have the maximum occurrence\n",
        "        top_candidates = [pred for pred, count in counts.items() if count == max_count]\n",
        "        # Randomly select from the top candidates in case of a tie\n",
        "        vote_result = random.choice(top_candidates)\n",
        "        final_predictions.append(vote_result)\n",
        "\n",
        "    return np.array(final_predictions)\n",
        "\n",
        "\n",
        "# Replace your usage of hard_voting in the script with this updated function.\n",
        "#function to load images and labels\n",
        "def load_images_and_labels(label_file, image_folder, target_size=(50, 50)):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    # Read the label file and store the labels in a dictionary\n",
        "    label_dict = {}\n",
        "    with open(label_file, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 2:\n",
        "                image_id, label = parts\n",
        "                label_dict[image_id] = label\n",
        "\n",
        "    # Load images corresponding to the labels and resize them\n",
        "    for image_id, label in label_dict.items():\n",
        "        image_path = os.path.join(image_folder, f\"{image_id}.jpg\")\n",
        "        if os.path.exists(image_path):\n",
        "            img = cv2.imread(image_path, cv2.IMREAD_COLOR)  # Read the image\n",
        "            if img is not None:\n",
        "                img_resized = cv2.resize(img, target_size)  # Resize the image\n",
        "                images.append(img_resized)  # Append the resized image\n",
        "                labels.append(label)\n",
        "                print(f\"load image {image_id}.jpg\")\n",
        "            else:\n",
        "                print(f\"Failed to load image {image_id}.jpg.\")\n",
        "        else:\n",
        "            print(f\"Image file {image_id}.jpg not found.\")\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "#function to resize images\n",
        "def resize_images(images, target_size=(50, 50)):\n",
        "    resized_images = []\n",
        "    for img in images:\n",
        "        if img is not None:\n",
        "            resized_img = cv2.resize(img, target_size)  # Resize the image\n",
        "            resized_images.append(resized_img)  # Append the resized image\n",
        "        else:\n",
        "            resized_images.append(None)  # Append None for any failed image loads\n",
        "    return resized_images\n",
        "\n",
        "#function to convert color images to grayscale\n",
        "def convert_to_grayscale(images):\n",
        "    grayscale_images = []\n",
        "    for img in images:\n",
        "        if img is not None:\n",
        "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "            grayscale_images.append(gray_img)  # Flatten the image\n",
        "        else:\n",
        "            grayscale_images.append(None)  # Append None for any failed image loads\n",
        "    return grayscale_images\n",
        "\n",
        "#function to flatten a list of images\n",
        "def flatten_images(image_list):\n",
        "    flattened_images = []\n",
        "    for img in image_list:\n",
        "        if img is not None:  # Ensure the image was loaded correctly\n",
        "            flattened = img.flatten()  # Flatten the image\n",
        "            flattened_images.append(flattened)\n",
        "        else:\n",
        "            flattened_images.append(None)  # Handle cases where an image might not be loaded\n",
        "    return flattened_images\n",
        "\n",
        "#funtion to normalize pixel values to specific range\n",
        "def normalize_images(images):\n",
        "    normalized_images = []\n",
        "    for img in images:\n",
        "        if img is not None:\n",
        "            normalized_img = img.astype(np.float32) / 255.0  # Normalize pixel values to [0, 1]\n",
        "            normalized_images.append(normalized_img)  # Append the normalized image\n",
        "        else:\n",
        "            normalized_images.append(None)  # Append None for any failed image loads\n",
        "    return normalized_images\n",
        "\n",
        "#image augementation function\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range = 10,\n",
        "       # shear_range = 0.2,\n",
        "       # zoom_range = 0.1,\n",
        "        horizontal_flip = True,\n",
        "        brightness_range=[0.8, 1.2],  # Adjust brightness between 80% and 120% of original\n",
        "        #contrast_range=[0.8, 1.2],      # Adjust contrast between 80% and 120% of original\n",
        "        width_shift_range= 0.05,  # Shift images horizontally by up to 10%\n",
        "        height_shift_range= 0.05,  # Shift images vertically by up to 10%\n",
        "        rescale=1./255,  # Normalize pixel values to the range [0, 1]\n",
        "        #target_size=(150, 150),  # Resize images to 150x150 pixels,\n",
        "        fill_mode='nearest'  # Use the nearest pixel value for filling\n",
        ")\n",
        "\n",
        "#function to load images with CascadeClassifier\n",
        "def cascade_load_images_and_labels(label_file, image_folder, target_size=(50, 50)):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    # Read the label file and store the labels in a dictionary\n",
        "    label_dict = {}\n",
        "    with open(label_file, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 2:\n",
        "                image_id,label = parts\n",
        "                label_dict[image_id] = label\n",
        "    haar_cascade = cv2.CascadeClassifier(MODEL_PATH+\"/haarcascade_frontalface_default.xml\")\n",
        "    if haar_cascade.empty():\n",
        "        print(\"Error: Failed to load cascade classifier.\")\n",
        "    else:\n",
        "        print(\"Cascade classifier loaded successfully.\")\n",
        "    # Load images corresponding to the labels and resize them\n",
        "    for image_id, label in label_dict.items():\n",
        "        image_path = os.path.join(image_folder, f\"{image_id}.jpg\")\n",
        "        if os.path.exists(image_path):\n",
        "            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read the image\n",
        "            if img is not None:\n",
        "                faces = haar_cascade.detectMultiScale(img, 1.1, 9)\n",
        "\n",
        "                for (x, y, w, h) in faces:\n",
        "                #cv2.rectangle(img, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
        "                    face = img[y:y + h, x:x + w]\n",
        "                if face is None or img.size == 0:\n",
        "                    print(\"Error: Failed to load FACE.\")\n",
        "                else:\n",
        "                    print(\"FACE loaded successfully.\")\n",
        "                #cv2.imwrite('face.jpg', faces)\n",
        "                img_resized = cv2.resize(face, target_size)  # Resize the image\n",
        "                images.append(img_resized)  # Append the resized image\n",
        "                labels.append(label)\n",
        "\n",
        "                print(f\"load image {image_id}.jpg\")\n",
        "            else:\n",
        "                print(f\"Failed to load image {image_id}.jpg.\")\n",
        "        else:\n",
        "            print(f\"Image file {image_id}.jpg not found.\")\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "def cascade_load_images(image_folder, target_size=(50, 50)):\n",
        "    images = []\n",
        "\n",
        "    # List all jpg files in the directory and sort them numerically\n",
        "    image_files = [f for f in os.listdir(image_folder) if f.endswith('.jpg')]\n",
        "    image_files.sort(key=lambda x: int(x.split('.')[0]))  # Sorting numerically by file name\n",
        "\n",
        "    haar_cascade = cv2.CascadeClassifier(MODEL_PATH+\"/haarcascade_frontalface_default.xml\")\n",
        "    if haar_cascade.empty():\n",
        "        print(\"Error: Failed to load cascade classifier.\")\n",
        "    else:\n",
        "        print(\"Cascade classifier loaded successfully.\")\n",
        "\n",
        "    # Load images and resize them\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(image_folder, image_file)\n",
        "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read the image\n",
        "        if img is not None:\n",
        "            faces = haar_cascade.detectMultiScale(img, 1.1, 9)\n",
        "\n",
        "            for (x, y, w, h) in faces:\n",
        "            #cv2.rectangle(img, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
        "                face = img[y:y + h, x:x + w]\n",
        "            if face is None or img.size == 0:\n",
        "                print(\"Error: Failed to load FACE.\")\n",
        "            else:\n",
        "                print(\"FACE loaded successfully.\")\n",
        "            #cv2.imwrite('face.jpg', faces)\n",
        "            img_resized = cv2.resize(face, target_size)  # Resize the image\n",
        "            images.append(img_resized)  # Append the resized image\n",
        "            print(f\"Loaded image {image_file}\")\n",
        "        else:\n",
        "            print(f\"Failed to load image {image_file}.\")\n",
        "\n",
        "    return images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGOylD6iQ5aF"
      },
      "source": [
        "#### Perform data augmentation (only run for training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhMhr9_0Qw5e",
        "outputId": "23ae7099-2765-4f6b-f7ec-793d5883a66c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "augmenting data\n",
            "data augmentation done\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "cnt = 0;\n",
        "\n",
        "train_image_map ={}\n",
        "train_image_cnt={}\n",
        "image_folder = TRAIN_DATA_PATH\n",
        "label_file =  os.path.join(image_folder, 'labels.txt') ;\n",
        "file1 = open(label_file, 'r')\n",
        "labels = []\n",
        "\n",
        "for line in file1:\n",
        "  parts = line.split(' ')\n",
        "  key = parts[1].strip()\n",
        "  if key not in train_image_cnt:\n",
        "    train_image_cnt[key] = 1  # Initialize the count for a new key\n",
        "  else:\n",
        "    train_image_cnt[key] += 1  # Increment the count for an existing key\n",
        "  train_image_map[parts[0].strip()] = parts[1].strip()\n",
        "\n",
        "if not os.path.exists(AUGMENTATION_DATA_PATH):\n",
        "    os.makedirs(AUGMENTATION_DATA_PATH)\n",
        "\n",
        "print(\"augmenting data\")\n",
        "#creating augmented images\n",
        "for  img_orignal,label in train_image_map.items():\n",
        "\n",
        "   # print(\"SIM  \", cnt);\n",
        "    images = image_folder + '/' + img_orignal + '.jpg'\n",
        "    #print(images)\n",
        "\n",
        "    # Loading a sample image\n",
        "    img = load_img(images)\n",
        "\n",
        "    # Converting the input sample image to an array\n",
        "    x = img_to_array(img)\n",
        "    # Reshaping the input image\n",
        "    x = x.reshape((1, ) + x.shape)\n",
        "\n",
        "    i = 0\n",
        "\n",
        "\n",
        "    for batch in datagen.flow(x, batch_size = 1,\n",
        "                             save_to_dir =AUGMENTATION_DATA_PATH + \"/\",\n",
        "                             save_prefix = label, save_format ='jpg'):\n",
        "\n",
        "      i += 1\n",
        "      if i > 5:\n",
        "          break\n",
        "    cnt +=1;\n",
        "\n",
        "#label augmented images\n",
        "files = os.listdir(AUGMENTATION_DATA_PATH)\n",
        "fid = open(AUGMENTATION_DATA_PATH + '/augmented_labels.txt', 'w')\n",
        "\n",
        "for f in files:\n",
        "  if f.endswith('.jpg'):\n",
        "    parts = f.split('_')\n",
        "    name = f.split('.')[0]\n",
        "    fid.write(\"{0} {1}\\n\".format(name,parts[0]))\n",
        "\n",
        "fid.close()\n",
        "print(\"data augmentation done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLxSEYGLQrRB"
      },
      "source": [
        "#### Read training/validation images and labels (only run for training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "Skc9-sp9Fpcd",
        "outputId": "ea610157-2af7-4c0c-8c44-04938374f364"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Failed to load cascade classifier.\n"
          ]
        },
        {
          "ename": "error",
          "evalue": "OpenCV(4.8.0) /io/opencv/modules/objdetect/src/cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'detectMultiScale'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6b596c294eb0>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#validation_images, validation_labels = load_images_and_labels(VALIDATION_DATA_PATH+\"/labels.txt\", VALIDATION_DATA_PATH)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#train_images, train_labels = cascade_load_images_and_labels(TRAIN_DATA_PATH+\"/labels.txt\", TRAIN_DATA_PATH)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcascade_load_images_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAUGMENTATION_DATA_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/augmented_labels.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAUGMENTATION_DATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mvalidation_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcascade_load_images_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVALIDATION_DATA_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/labels.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVALIDATION_DATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-ad39ecb3759c>\u001b[0m in \u001b[0;36mcascade_load_images_and_labels\u001b[0;34m(label_file, image_folder, target_size)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Read the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhaar_cascade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv/modules/objdetect/src/cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'detectMultiScale'\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "#train_images, train_labels = load_images_and_labels(AUGMENTATION_DATA_PATH+\"/augmented_labels.txt\", AUGMENTATION_DATA_PATH)\n",
        "#validation_images, validation_labels = load_images_and_labels(VALIDATION_DATA_PATH+\"/labels.txt\", VALIDATION_DATA_PATH)\n",
        "#train_images, train_labels = cascade_load_images_and_labels(TRAIN_DATA_PATH+\"/labels.txt\", TRAIN_DATA_PATH)\n",
        "train_images, train_labels = cascade_load_images_and_labels(AUGMENTATION_DATA_PATH+\"/augmented_labels.txt\", AUGMENTATION_DATA_PATH)\n",
        "validation_images, validation_labels = cascade_load_images_and_labels(VALIDATION_DATA_PATH+\"/labels.txt\", VALIDATION_DATA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0ROVbhbRZsu"
      },
      "source": [
        "### Train and tune model (only run for training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED-pTm-XQxoe"
      },
      "source": [
        "#### Perform data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXa1sQvAQw7W"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "X_train = flatten_images(train_images)\n",
        "X_validation = flatten_images(validation_images)\n",
        "\n",
        "y_train = train_labels\n",
        "y_validation = validation_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRViFUGGBMsg"
      },
      "source": [
        "#### SVM Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96U4VU9zBPqS"
      },
      "outputs": [],
      "source": [
        "# Initialize the StandardScaler\n",
        "svm_sc = StandardScaler()\n",
        "svm_sc.fit(X_train)\n",
        "model_path = os.path.join(MODEL_PATH, 'svm_sc_model.joblib')\n",
        "dump(svm_sc, model_path)\n",
        "X_svm_train = sc.transform(X_train)\n",
        "X_svm_validation = sc.transform(X_validation)\n",
        "\n",
        "#using pca to reduce dimention\n",
        "svm_pca = PCA(n_components=160)\n",
        "svm_pca.fit(X_train,y_train)\n",
        "# Save the model to the specified path\n",
        "model_path = os.path.join(MODEL_PATH, 'svm_pca_model.joblib')\n",
        "dump(svm_pca, model_path)\n",
        "X_svm_train = svm_pca.transform(X_svm_train)\n",
        "X_svm_validation = svm_pca.transform(X_svm_validation)\n",
        "\n",
        "#using lda to reduce dimention\n",
        "svm_lda = LinearDiscriminantAnalysis(n_components=26)\n",
        "svm_lda.fit(X_svm_train, y_train)\n",
        "model_path = os.path.join(MODEL_PATH, 'svm_lda_model.joblib')\n",
        "dump(svm_lda, model_path)\n",
        "\n",
        "X_svm_train = svm_lda.transform(X_svm_train)\n",
        "X_svm_validation = svm_lda.transform(X_svm_validation)\n",
        "\n",
        "svm_classifier = OneVsOneClassifier(SVC(kernel='rbf', C= 10, gamma = 0.001))\n",
        "\n",
        "svm_classifier.fit(X_svm_train, y_train)\n",
        "\n",
        "model_path = os.path.join(MODEL_PATH, 'svm_model.joblib')\n",
        "dump(svm_classifier, model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGQkSB3ACYLJ"
      },
      "source": [
        "#### KNN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ereKCKjxCZ-Z"
      },
      "outputs": [],
      "source": [
        "# Initialize the StandardScaler\n",
        "knn_sc = StandardScaler()\n",
        "knn_sc.fit(X_train)\n",
        "model_path = os.path.join(MODEL_PATH, 'knn_sc_model.joblib')\n",
        "dump(knn_sc, model_path)\n",
        "X_knn_train = knn_sc.transform(X_train)\n",
        "X_knn_validation = knn_sc.transform(X_validation)\n",
        "\n",
        "# Using PCA to reduce dimensionality\n",
        "knn_pca = PCA(n_components=180)\n",
        "knn_pca.fit(X_knn_train, y_train)\n",
        "# Save the PCA model to the specified path\n",
        "model_path = os.path.join(MODEL_PATH, 'knn_pca_model.joblib')\n",
        "dump(knn_pca, model_path)\n",
        "X_knn_train = knn_pca.transform(X_knn_train)\n",
        "X_knn_validation = knn_pca.transform(X_knn_validation)\n",
        "\n",
        "#using lda to reduce dimention\n",
        "knn_lda = LinearDiscriminantAnalysis()\n",
        "knn_lda.fit(X_knn_train, y_train)\n",
        "model_path = os.path.join(MODEL_PATH, 'knn_lda_model.joblib')\n",
        "dump(knn_lda, model_path)\n",
        "\n",
        "X_knn_train = knn_lda.transform(X_knn_train)\n",
        "X_knn_validation = knn_lda.transform(X_knn_validation)\n",
        "\n",
        "# Initialize and train the KNN classifier\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_classifier.fit(X_knn_train, y_train)\n",
        "\n",
        "# Save the model to the specified path\n",
        "model_path = os.path.join(MODEL_PATH, 'knn_model.joblib')\n",
        "dump(knn_classifier, model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9-AEZNdCe4H"
      },
      "source": [
        "#### Logistic Regression Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATeVJw2xCvvt"
      },
      "outputs": [],
      "source": [
        "# Initialize the StandardScaler\n",
        "logreg_sc = StandardScaler()\n",
        "logreg_sc.fit(X_train)\n",
        "model_path = os.path.join(MODEL_PATH, 'logreg_sc_model.joblib')\n",
        "dump(logreg_sc, model_path)\n",
        "X_logreg_train = logreg_sc.transform(X_train)\n",
        "X_logreg_validation = logreg_sc.transform(X_validation)\n",
        "\n",
        "# Using PCA to reduce dimensionality\n",
        "logreg_pca = PCA(n_components=180)\n",
        "logreg_pca.fit(X_logreg_train, y_train)\n",
        "# Save the PCA model to the specified path\n",
        "model_path = os.path.join(MODEL_PATH, 'logreg_pca_model.joblib')\n",
        "dump(logreg_pca, model_path)\n",
        "X_logreg_train = logreg_pca.transform(X_logreg_train)\n",
        "X_logreg_validation = logreg_pca.transform(X_logreg_validation)\n",
        "\n",
        "#using lda to reduce dimention\n",
        "logreg_lda = LinearDiscriminantAnalysis()\n",
        "logreg_lda.fit(X_logreg_train, y_train)\n",
        "model_path = os.path.join(MODEL_PATH, 'logreg_lda_model.joblib')\n",
        "dump(logreg_lda, model_path)\n",
        "\n",
        "X_logreg_train = logreg_lda.transform(X_logreg_train)\n",
        "X_logreg_validation = logreg_lda.transform(X_logreg_validation)\n",
        "\n",
        "\n",
        "# Initialize and train the Logistic Regression classifier\n",
        "logreg_classifier = LogisticRegression(C=0.1,max_iter=10000)\n",
        "logreg_classifier.fit(X_logreg_train, y_train)\n",
        "\n",
        "# Save the model to the specified path\n",
        "model_path = os.path.join(MODEL_PATH, 'logreg_model.joblib')\n",
        "dump(logreg_classifier, model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfgdIKnaCl1T"
      },
      "source": [
        "#### Random Forest Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr0dYY5GCi6S"
      },
      "outputs": [],
      "source": [
        "# Initialize the StandardScaler\n",
        "rf_sc = StandardScaler()\n",
        "rf_sc.fit(X_train)\n",
        "model_path = os.path.join(MODEL_PATH, 'rf_sc_model.joblib')\n",
        "dump(rf_sc, model_path)\n",
        "X_rf_train = rf_sc.transform(X_train)\n",
        "X_rf_validation = rf_sc.transform(X_validation)\n",
        "\n",
        "# Using PCA to reduce dimensionality\n",
        "rf_pca = PCA(n_components=180)\n",
        "rf_pca.fit(X_rf_train, y_train)\n",
        "# Save the PCA model to the specified path\n",
        "model_path = os.path.join(MODEL_PATH, 'rf_pca_model.joblib')\n",
        "dump(rf_pca, model_path)\n",
        "X_rf_train = rf_pca.transform(X_rf_train)\n",
        "X_rf_validation = rf_pca.transform(X_rf_validation)\n",
        "\n",
        "#using lda to reduce dimention\n",
        "rf_lda = LinearDiscriminantAnalysis()\n",
        "rf_lda.fit(X_rf_train, y_train)\n",
        "model_path = os.path.join(MODEL_PATH, 'rf_lda_model.joblib')\n",
        "dump(rf_lda, model_path)\n",
        "\n",
        "X_rf_train = rf_lda.transform(X_rf_train)\n",
        "X_rf_validation = rf_lda.transform(X_rf_validation)\n",
        "\n",
        "# Initialize and train the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=500,max_depth=300)\n",
        "rf_classifier.fit(X_rf_train, y_train)\n",
        "\n",
        "# Save the model to the specified path\n",
        "model_path = os.path.join(MODEL_PATH, 'rf_model.joblib')\n",
        "dump(rf_classifier, model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GwAgEtTR4Ub"
      },
      "source": [
        "### Generate model performance reports (only run for training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc2fhBGVF-Ku"
      },
      "source": [
        "#### SVM Confusion Matrix and Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rocnyGNjR3tX"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "loaded_svm_classifier = load(os.path.join(MODEL_PATH, 'svm_model.joblib'))\n",
        "\n",
        "\n",
        "# Evaluate the loaded model on training set\n",
        "y_pred = loaded_svm_classifier.predict(X_svm_train)\n",
        "cm = confusion_matrix(y_train, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=loaded_svm_classifier.classes_)\n",
        "disp.plot()\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(y_train, y_pred)\n",
        "print(\"svm model accuracy on training set:\", accuracy)\n",
        "\n",
        "# Evaluate the loaded model on validation set\n",
        "y_pred = loaded_svm_classifier.predict(X_svm_validation)\n",
        "cm = confusion_matrix(y_validation, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=loaded_svm_classifier.classes_)\n",
        "disp.plot()\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(y_validation, y_pred)\n",
        "print(\"svm model accuracy on validation set:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVlm4Mh2GFYV"
      },
      "source": [
        "#### Random Forest Confusion Matrix and Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1LGfAUzGL_t"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "loaded_rf_classifier = load(os.path.join(MODEL_PATH, 'rf_model.joblib'))\n",
        "\n",
        "# Evaluate the loaded model on training set\n",
        "y_pred = loaded_rf_classifier.predict(X_rf_train)\n",
        "cm = confusion_matrix(y_train, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=loaded_rf_classifier.classes_)\n",
        "disp.plot()\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(y_train, y_pred)\n",
        "print(\"Random Forest model accuracy on training set:\", accuracy)\n",
        "\n",
        "# Evaluate the loaded model on validation set\n",
        "y_pred = loaded_rf_classifier.predict(X_rf_validation)\n",
        "cm = confusion_matrix(y_validation, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=loaded_rf_classifier.classes_)\n",
        "disp.plot()\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(y_validation, y_pred)\n",
        "print(\"Random Forest model accuracy on validation set:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zor7JOEGgKg"
      },
      "source": [
        "#### KNN Confusion Matrix and Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdWXFHQSGjYK"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "loaded_knn_classifier = load(os.path.join(MODEL_PATH, 'knn_model.joblib'))\n",
        "\n",
        "# Evaluate the loaded model on training set\n",
        "y_pred = loaded_knn_classifier.predict(X_knn_train)\n",
        "cm = confusion_matrix(y_train, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=loaded_knn_classifier.classes_)\n",
        "disp.plot()\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(y_train, y_pred)\n",
        "print(\"KNN model accuracy on training set:\", accuracy)\n",
        "\n",
        "# Evaluate the loaded model on validation set\n",
        "y_pred = loaded_knn_classifier.predict(X_knn_validation)\n",
        "cm = confusion_matrix(y_validation, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=loaded_knn_classifier.classes_)\n",
        "disp.plot()\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(y_validation, y_pred)\n",
        "print(\"KNN model accuracy on validation set:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51YZSHaAG5pt"
      },
      "source": [
        "#### Logistic Regression Confusion Matrix and Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzAHiTTlHCtY"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "loaded_logreg_classifier = load(os.path.join(MODEL_PATH, 'logreg_model.joblib'))\n",
        "\n",
        "# Evaluate the loaded model on training set\n",
        "y_pred = loaded_logreg_classifier.predict(X_logreg_train)\n",
        "cm = confusion_matrix(y_train, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=loaded_logreg_classifier.classes_)\n",
        "disp.plot()\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(y_train, y_pred)\n",
        "print(\"Logistic Regression accuracy on training set:\", accuracy)\n",
        "\n",
        "# Evaluate the loaded model on validation set\n",
        "y_pred = loaded_logreg_classifier.predict(X_logreg_validation)\n",
        "cm = confusion_matrix(y_validation, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=loaded_logreg_classifier.classes_)\n",
        "disp.plot()\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(y_validation, y_pred)\n",
        "print(\"Logistic Regression model accuracy on validation set:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzHGiFyLHcVA"
      },
      "source": [
        "#### Ensemble Confusion Matrix and Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xAFFkJJpIYf"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "loaded_svm_classifier = load(os.path.join(MODEL_PATH, 'svm_model.joblib'))\n",
        "y_svm_pred = loaded_svm_classifier.predict(X_svm_train)\n",
        "\n",
        "loaded_rf_classifier = load(os.path.join(MODEL_PATH, 'rf_model.joblib'))\n",
        "y_rf_pred = loaded_rf_classifier.predict(X_rf_train)\n",
        "\n",
        "loaded_knn_classifier = load(os.path.join(MODEL_PATH, 'knn_model.joblib'))\n",
        "y_knn_pred = loaded_knn_classifier.predict(X_knn_train)\n",
        "\n",
        "loaded_logreg_classifier = load(os.path.join(MODEL_PATH, 'logreg_model.joblib'))\n",
        "y_logreg_pred = loaded_logreg_classifier.predict(X_logreg_train)\n",
        "\n",
        "y_pred = hard_voting(y_svm_pred,y_rf_pred,y_knn_pred,y_logreg_pred)\n",
        "\n",
        "cm = confusion_matrix(y_train, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=loaded_logreg_classifier.classes_)\n",
        "disp.plot()\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(y_train, y_pred)\n",
        "print(\"Ensemble accuracy on training set:\", accuracy)\n",
        "\n",
        "# Evaluate the loaded model on validation set\n",
        "y_svm_pred = loaded_svm_classifier.predict(X_svm_validation)\n",
        "\n",
        "y_rf_pred = loaded_rf_classifier.predict(X_rf_validation)\n",
        "\n",
        "y_knn_pred = loaded_knn_classifier.predict(X_knn_validation)\n",
        "\n",
        "y_logreg_pred = loaded_logreg_classifier.predict(X_logreg_validation)\n",
        "\n",
        "y_pred = hard_voting(y_svm_pred,y_rf_pred,y_knn_pred,y_logreg_pred)\n",
        "\n",
        "cm = confusion_matrix(y_validation, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=loaded_logreg_classifier.classes_)\n",
        "disp.plot()\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(y_validation, y_pred)\n",
        "print(\"Ensemble model accuracy on validation set:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNkewk5FSoZT"
      },
      "source": [
        "### Read test images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "qXMUAGwcSn9V",
        "outputId": "b06ef14f-e83d-47a7-c517-757e7ddbe486"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/SP24-CSEN240-2/Project/Test'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-78063e97339d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#testing_images = cascade_load_images(VALIDATION_DATA_PATH)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtesting_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcascade_load_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_DATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-ad39ecb3759c>\u001b[0m in \u001b[0;36mcascade_load_images\u001b[0;34m(image_folder, target_size)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m# List all jpg files in the directory and sort them numerically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mimage_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_folder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0mimage_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sorting numerically by file name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/SP24-CSEN240-2/Project/Test'"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "testing_images = cascade_load_images(TEST_DATA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CsU5LHrRjk6"
      },
      "source": [
        "### Generate prediction file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAfb-WjBQw0r",
        "outputId": "68ef734d-7e0e-4841-8a04-7bbfb145a33d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions written to /content/drive/MyDrive/SP24-CSEN240-2/Project/Prediction/labels.txt\n"
          ]
        }
      ],
      "source": [
        "# TODO Generate a group-x.txt file with predictions and save it to OUTPUT_PATH folder\n",
        "\n",
        "X_testing = flatten_images(testing_images)\n",
        "\n",
        "\n",
        "\n",
        "#scale\n",
        "loaded_svm_sc = load(os.path.join(MODEL_PATH, 'svm_sc_model.joblib'))\n",
        "X_svm_testing = loaded_svm_sc.transform(X_testing)\n",
        "#pca\n",
        "loaded_svm_pca = load(os.path.join(MODEL_PATH, 'svm_pca_model.joblib'))\n",
        "X_svm_testing = loaded_svm_pca.transform(X_svm_testing)\n",
        "#lda\n",
        "loaded_svm_lda = load(os.path.join(MODEL_PATH, 'svm_lda_model.joblib'))\n",
        "X_svm_testing = loaded_svm_lda.transform(X_svm_testing)\n",
        "#svm prediction\n",
        "loaded_svm_classifier = load(os.path.join(MODEL_PATH, 'svm_model.joblib'))\n",
        "y_svm_pred = loaded_svm_classifier.predict(X_svm_testing)\n",
        "\n",
        "#scale\n",
        "loaded_rf_sc = load(os.path.join(MODEL_PATH, 'rf_sc_model.joblib'))\n",
        "X_rf_testing = loaded_rf_sc.transform(X_testing)\n",
        "#pca\n",
        "loaded_rf_pca = load(os.path.join(MODEL_PATH, 'rf_pca_model.joblib'))\n",
        "X_rf_testing = loaded_rf_pca.transform(X_rf_testing)\n",
        "#lda\n",
        "loaded_rf_lda = load(os.path.join(MODEL_PATH, 'rf_lda_model.joblib'))\n",
        "X_rf_testing = loaded_rf_lda.transform(X_rf_testing)\n",
        "#rf prediction\n",
        "loaded_rf_classifier = load(os.path.join(MODEL_PATH, 'rf_model.joblib'))\n",
        "y_rf_pred = loaded_rf_classifier.predict(X_rf_testing)\n",
        "\n",
        "#scale\n",
        "loaded_knn_sc = load(os.path.join(MODEL_PATH, 'knn_sc_model.joblib'))\n",
        "X_knn_testing = loaded_knn_sc.transform(X_testing)\n",
        "#pca\n",
        "loaded_knn_pca = load(os.path.join(MODEL_PATH, 'knn_pca_model.joblib'))\n",
        "X_knn_testing = loaded_knn_pca.transform(X_knn_testing)\n",
        "#lda\n",
        "loaded_knn_lda = load(os.path.join(MODEL_PATH, 'knn_lda_model.joblib'))\n",
        "X_knn_testing = loaded_knn_lda.transform(X_knn_testing)\n",
        "#knn prediction\n",
        "loaded_knn_classifier = load(os.path.join(MODEL_PATH, 'knn_model.joblib'))\n",
        "y_knn_pred = loaded_knn_classifier.predict(X_knn_testing)\n",
        "\n",
        "#scale\n",
        "loaded_logreg_sc = load(os.path.join(MODEL_PATH, 'logreg_sc_model.joblib'))\n",
        "X_logreg_testing = loaded_logreg_sc.transform(X_testing)\n",
        "#pca\n",
        "loaded_logreg_pca = load(os.path.join(MODEL_PATH, 'logreg_pca_model.joblib'))\n",
        "X_logreg_testing = loaded_logreg_pca.transform(X_logreg_testing)\n",
        "#lda\n",
        "loaded_logreg_lda = load(os.path.join(MODEL_PATH, 'logreg_lda_model.joblib'))\n",
        "X_logreg_testing = loaded_logreg_lda.transform(X_logreg_testing)\n",
        "#logreg prediction\n",
        "loaded_logreg_classifier = load(os.path.join(MODEL_PATH, 'logreg_model.joblib'))\n",
        "y_logreg_pred = loaded_logreg_classifier.predict(X_logreg_testing)\n",
        "\n",
        "y_pred = hard_voting(y_svm_pred,y_rf_pred,y_knn_pred,y_logreg_pred)\n",
        "\n",
        "write_predictions_to_file(y_pred,PREDICTION_FILE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLHltjrQyWPB"
      },
      "source": [
        "###Accuracy measurement script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUf4PIhzyV4G",
        "outputId": "346ee3c7-7545-42c4-9b3c-dd29cc5ac99c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy score: 0.9281437125748503\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "with open(TEST_LABEL_PATH, 'r') as f:\n",
        "    y_test = [line.strip().split(' ')[1] for line in f.readlines()]\n",
        "\n",
        "with open(PREDICTION_FILE_PATH, 'r') as f:\n",
        "    y_pred = [line.strip().split(' ')[1] for line in f.readlines()]\n",
        "\n",
        "score = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy score: %s'% score)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gGOylD6iQ5aF",
        "-GwAgEtTR4Ub"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
